---
layout: default
title: Research
---
<h1>Selected work</h1>
<ul>
  <li><b>Open Foundation Models</b>
    <br>Rishi Bommasani*, Sayash Kapoor* et al.
    <br><b>Science 2024 & ICML 2024 (Oral)</b>
    <br><a href="https://www.science.org/doi/10.1126/science.adp1848">[Science Paper]</a> <a href="https://arxiv.org/abs/2403.07918">[ICML Paper]</a> <a href="https://crfm.stanford.edu/open-fms/">[Website]</a>
  </li>
  <li><b>The Foundation Model Transparency Index</b>
    <br>Rishi Bommasani*, Kevin Klyman* et al.
    <br><b>TMLR 2024 (Outstanding Paper)</b>
    <br><a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
  </li>
  <li><b>Holistic Evaluation of Language Models</b>
    <br>Percy Liang*, Rishi Bommasani*, Tony Lee* et al.
    <br><b>TMLR 2023 (Best Paper)</b>
    <br><a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
  </li>
  <li><b>On the Opportunities and Risks of Foundation Models</b>
    <br>Rishi Bommasani*, ..., Percy Liang*
    <br><a href="https://crfm.stanford.edu/report.html">[Paper]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
  </li>
</ul>

<h1>Full list</h1>
<ul>
  <li>
    <b>Advancing Science- and Evidence-based AI Policy</b>
    <br>Rishi Bommasani et al.
    <br><b>Science 2025</b>
    <br><a href="https://www.science.org/doi/10.1126/science.adu8449">[Paper]</a>
  </li>
  
  <li>
    <b>The 2025 Foundation Model Transparency Index</b>
    <br>Alexander Wan et al.
    <br><a href="https://arxiv.org/abs/2512.10169">[Paper]</a>
  </li>
  
  <li>
    <b>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</b>
    <br>Sayash Kapoor et al.
    <br><a href="https://arxiv.org/abs/2510.11977">[Paper]</a>
  </li>
  
  <li>
    <b>NeurIPS should lead scientific consensus on AI policy</b>
    <br>Rishi Bommasani
    <br><b>NeurIPS 2025</b>
  </li>
  
  <li>
    <b>Do Companies Make Good on their Voluntary Commitments to the White House?</b>
    <br>Jennifer Wang, Kayla Huang, Kevin Klyman, Rishi Bommasani
    <br><b>AIES 2025</b>
    <br><a href="https://arxiv.org/abs/2508.08345">[Paper]</a>
  </li>
  
  <li>
    <b>Deconstructing Provider and Deployer Obligations for Fairness in General-Purpose AI</b>
    <br>Vyoma Raman et al.
    <br><b>AIES 2025</b>
  </li>
  
  <li>
    <b>Reliable and Responsible Foundation Models</b>
    <br>Xinyu Yang et al.
    <br><b>TMLR 2025 (Outstanding Survey Paper)</b>
    <br><a href="https://openreview.net/forum?id=nLJZh4M6S5">[Paper]</a>
  </li>
  
  <li>
    <b>International AI Safety Report</b>
    <br>Yoshua Bengio et al.
    <br><b>France AI Action Summit</b>
    <br><a href="https://arxiv.org/abs/2501.17805">[Paper]</a>
  </li>

  <li><b>In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI</b>
    <br>Shayne Longpre et al.
    <br><b>ICML 2025 (Spotlight, top 2.5% of papers)</b>
    <br><a href="https://arxiv.org/abs/2503.16861">[Paper]</a>
  </li>
  <li><b>Language model developers should report train-test overlap</b>
    <br>Andy Zhang et al.
    <br><b>ICML 2025 (Spotlight, top 2.5% of papers)</b>
    <br><a href="https://arxiv.org/abs/2410.08385">[Paper]</a>
  </li>
  <li><b>Toward an Evaluation Science for Generative AI Systems</b>
    <br>Laura Weidinger*, Inioluwa Deborah Raji*, et al.
    <br><b>National Academy of Engineering 2025</b>
    <br><a href="https://arxiv.org/abs/2503.05336">[Paper]</a>
  </li>
  <li><b>Beyond Release: Access Considerations for Generative AI Systems</b>
    <br>Irene Solaiman, Rishi Bommasani et al.
    <br><a href="https://arxiv.org/abs/2502.16701">[Paper]</a>
  </li>
  <li><b>The Reality of AI and Biorisk</b>
    <br>Aidan Peppin et al.
    <br><b>FAccT 2025</b>
    <br><a href="https://arxiv.org/abs/2412.01946">[Paper]</a>
  </li>
  <li><b>Considerations for Governing Open Foundation Models</b>
    <br>Rishi Bommasani et al.
    <br><b>Science 2024</b>
    <br><a href="https://www.science.org/doi/10.1126/science.adp1848">[Paper]</a>
  </li>
  <li><b>Effective Mitigations for Systemic Risks from General-Purpose AI</b>
    <br>Risto Uuk et al.
    <br><a href="https://arxiv.org/abs/2412.02145">[Paper]</a>
  </li>
  <li><b>The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources</b>
    <br>Shayne Longpre*, Stella Biderman* et al.
    <br><b>TMLR 2024 (Outstanding Survey Paper)</b>
    <br><a href="https://arxiv.org/abs/2406.16746">[Paper]</a> <a href="https://fmcheatsheet.org/">[Website]</a>
  </li>
  <li><b>Interim International Scientific Report on the Safety of Advanced AI</b>
    <br>Yoshua Bengio et al.
    <br><b>2024 AI Seoul Summit</b>
    <br><a href="https://assets.publishing.service.gov.uk/media/66474eab4f29e1d07fadca3d/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf">[Paper]</a> <a href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai">[Website]</a>
  </li>
  <li><b>The 2024 Foundation Model Transparency Index</b>
    <br>Rishi Bommasani*, Kevin Klyman* et al.
    <br><b>TMLR 2024</b>
    <br><a href="https://crfm.stanford.edu/fmti/paper.pdf">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
  </li>
  <li><b>On the Societal Impact of Open Foundation Models</b>
    <br>Sayash Kapoor*, Rishi Bommasani* et al.
    <br><b>ICML 2024 (Oral, top 1.5% of papers)</b>
    <br><a href="https://arxiv.org/abs/2403.07918">[Paper]</a> <a href="https://crfm.stanford.edu/open-fms/">[Website]</a>
  </li>
  <li><b>A Safe Harbor for AI Evaluation and Red Teaming</b>
    <br>Shayne Longpre et al.
    <br><b>ICML 2024 (Oral, top 1.5% of papers)</b>
    <br><a href="https://arxiv.org/abs/2403.04893">[Paper]</a> <a href="https://sites.mit.edu/ai-safe-harbor/">[Website]</a>
  </li>
  <li><b>Foundation Model Transparency Reports</b>
    <br>Rishi Bommasani et al.
    <br><b>AIES 2024 (Oral, top 1.5% of papers)</b>
    <br><a href="https://arxiv.org/abs/2402.16268">[Paper]</a>
  </li>
  <li><b>Ecosystem Graphs: The Social Footprint of Foundation Models</b>
    <br>Rishi Bommasani et al.
    <br><b>AIES 2024</b>
    <br><a href="https://arxiv.org/abs/2303.15772">[Paper]</a> <a href="https://crfm.stanford.edu/ecosystem-graphs/">[Website]</a> <a href="https://crfm.stanford.edu/2023/03/29/ecosystem-graphs.html">[Blog]</a> <a href="https://github.com/stanford-crfm/ecosystem-graphs">[Code]</a>
  </li>
  <li><b>Trustworthy Social Bias Measurement</b>
    <br>Rishi Bommasani, Percy Liang
    <br><b>AIES 2024</b>
    <br><a href="https://arxiv.org/abs/2212.11672">[Paper]</a> <a href="https://github.com/rishibommasani/BiasMeasures">[Code]</a>
  </li>
  <li><b>AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing</b>
    <br>Neel Guha*, Christie M. Lawrence* et al.
    <br><b>George Washington Law Review 2024</b>
    <br><a href="https://hai.stanford.edu/sites/default/files/2023-11/AI-Regulatory-Alignment.pdf">[Paper]</a> <a href="https://hai.stanford.edu/policy-brief-ai-regulatory-alignment-problem">[Policy Brief]</a>
  </li>
<li><b> The 2023 Foundation Model Transparency Index </b>
  <br> Rishi Bommasani*, Kevin Klyman* et al.
  <br> <b> TMLR 2024 (Outstanding Paper)</b>
  <br> <a href="https://arxiv.org/abs/2310.12941">[Paper]</a> <a href="https://crfm.stanford.edu/fmti/">[Website]</a> <a href="https://www.aisnakeoil.com/p/how-transparent-are-foundation-model">[Blog]</a> <a href="https://github.com/stanford-crfm/fmti">[Data]</a>
</li>
  <li><b>Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes</b>
    <br>Connor Toups*, Rishi Bommasani* et al.
    <br><b>NeurIPS 2023</b>
    <br><a href="https://arxiv.org/abs/2307.05862">[Paper]</a> <a href="https://github.com/rishibommasani/EcosystemLevelAnalysis">[Code]</a>
  </li>
  <li><b>Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs</b>
    <br>Deepak Narayanan et al.
    <br><b>NeurIPS 2023</b>
    <br><a href="https://arxiv.org/abs/2305.02440">[Paper]</a>
  </li>
  <li><b>Evaluation for Change</b>
    <br>Rishi Bommasani
    <br><b>ACL 2023</b>
    <br><a href="https://arxiv.org/abs/2212.11670">[Paper]</a>
  </li>
  <li><b>Evaluating Human-Language Model Interaction</b>
    <br>Mina Lee et al.
    <br><b>TMLR 2023</b>
    <br><a href="https://arxiv.org/abs/2212.09746">[Paper]</a> <a href="https://github.com/minggg/halie">[Code]</a>
  </li>
  <li><b>Holistic Evaluation of Language Models</b>
    <br>Percy Liang*, Rishi Bommasani*, Tony Lee* et al.
    <br><b>TMLR 2023 (Best Paper)</b>
    <br><a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://crfm.stanford.edu/helm/">[Website]</a> <a href="https://crfm.stanford.edu/2022/11/17/helm.html">[Blog]</a> <a href="https://github.com/stanford-crfm/helm">[Code]</a>
  </li>
  <li><b>Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?</b>
    <br>Rishi Bommasani et al.
    <br><b>NeurIPS 2022</b>
    <br><a href="https://arxiv.org/abs/2211.13972">[Paper]</a> <a href="https://github.com/rishibommasani/HomogenizationNeurIPS2022">[Code]</a>
  </li>
  <li><b>Emergent Abilities of Large Language Models</b>
    <br>Jason Wei et al.
    <br><b>TMLR 2022 (Outstanding Survey Paper)</b>
    <br><a href="https://arxiv.org/abs/2206.07682">[Paper]</a> <a href="https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models">[Blog]</a>
  </li>
  <li><b>Data Governance in the Age of Large-Scale Data-Driven Language Technology</b>
    <br>Yacine Jernite et al.
    <br><b>FAccT 2022</b>
    <br><a href="https://dl.acm.org/doi/abs/10.1145/3531146.3534637">[Paper]</a>
  </li>
  <li><b>The Time Is Now to Develop Community Norms for the Release of Foundation Models</b>
    <br>Percy Liang, Rishi Bommasani, Kathleen A. Creel, Rob Reich
    <br><a href="https://crfm.stanford.edu/2022/05/17/community-norms.html">[Blog]</a> <a href="https://www.protocol.com/enterprise/foundation-models-ai-standards-stanford?sf167186232=1">[Op-ed]</a>
  </li>
  <li><b>On the Opportunities and Risks of Foundation Models</b>
    <br>Rishi Bommasani*, <a href="https://crfm.stanford.edu/report.html">full list of authors</a>, Percy Liang*
    <br><a href="https://arxiv.org/abs/2108.07258">[arXiv]</a> <a href="https://www.youtube.com/watch?v=ZshcPdavsdU">[Explainer Video]</a> <a href="https://crfm.stanford.edu/2021/10/18/commentaries.html">[Commentaries]</a>
  </li>
  <li><b>Generalized Optimal Linear Orders</b>
    <br>Rishi Bommasani
    <br><b>M.S. Thesis, Cornell University</b>
    <br><a href="https://arxiv.org/abs/2108.10692">[arXiv]</a> <a href="https://ecommons.cornell.edu/handle/1813/103195">[Thesis]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/Thesis-Defense-2020.pdf">[Slides]</a>
  </li>
  <li><b>Intrinsic Evaluation of Summarization Datasets</b>
    <br>Rishi Bommasani, Claire Cardie
    <br><b>EMNLP 2020</b>
    <br><a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf">[Paper]</a> <a href="https://slideslive.com/38938755/intrinsic-evaluation-of-summarization-datasets">[Oral]</a> <a href="https://github.com/rishibommasani/rishibommasani.github.io/blob/master/papers/EMNLP-2020-Slides.pdf">[Slides]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.bib">[BibTeX]</a> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.649/">[Abstract]</a>
  </li>
  <li><b>Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings</b>
    <br>Rishi Bommasani, Kelly Davis, Claire Cardie
    <br><b>ACL 2020</b>
    <br><a href="https://www.aclweb.org/anthology/2020.acl-main.431.pdf">[Paper]</a> <a href="https://slideslive.com/38929398/interpreting-pretrained-contextualized-representations-via-reductions-to-static-embeddings">[Oral]</a> <a href="...
