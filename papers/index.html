---
layout: default
title: Research
---
<h1>Selected work</h1>
<ul>
  <li>
    <a href="https://www.science.org/doi/10.1126/science.adu8449"><b>Advancing Science- and Evidence-based AI Policy</b></a>
    <br>Rishi Bommasani et al.
    <br><b>Science 2025</b>
  </li>
  <li><a href="https://arxiv.org/abs/2403.07918"><b>Open Foundation Models</b></a>
    <br>Rishi Bommasani*, Sayash Kapoor* et al.
    <br><b>Science 2024 & ICML 2024 (Oral)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2310.12941"><b>The Foundation Model Transparency Index</b></a>
    <br>Rishi Bommasani*, Kevin Klyman* et al.
    <br><b>TMLR 2024 (Outstanding Paper)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2211.09110"><b>Holistic Evaluation of Language Models</b></a>
    <br>Percy Liang*, Rishi Bommasani*, Tony Lee* et al.
    <br><b>TMLR 2023 (Best Paper)</b>
  </li>
  <li><a href="https://crfm.stanford.edu/report.html"><b>On the Opportunities and Risks of Foundation Models</b></a>
    <br>Rishi Bommasani et al.
  </li>
</ul>

<h1>Full list</h1>
<ul>
  <li>
    <a href="https://www.science.org/doi/10.1126/science.adu8449"><b>Advancing Science- and Evidence-based AI Policy</b></a>
    <br>Rishi Bommasani et al.
    <br><b>Science 2025</b>
  </li>
  
  <li>
    <a href="https://arxiv.org/abs/2512.10169"><b>The 2025 Foundation Model Transparency Index</b></a>
    <br>Alexander Wan et al.
  </li>
  
  <li>
    <a href="https://arxiv.org/abs/2510.11977"><b>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</b></a>
    <br>Sayash Kapoor et al.
  </li>
  
  <li>
    <b>NeurIPS should lead scientific consensus on AI policy</b>
    <br>Rishi Bommasani
    <br><b>NeurIPS 2025</b>
  </li>
  
  <li>
    <a href="https://arxiv.org/abs/2508.08345"><b>Do Companies Make Good on their Voluntary Commitments to the White House?</b></a>
    <br>Jennifer Wang, Kayla Huang, Kevin Klyman, Rishi Bommasani
    <br><b>AIES 2025</b>
  </li>
  
  <li>
    <b>Deconstructing Provider and Deployer Obligations for Fairness in General-Purpose AI</b>
    <br>Vyoma Raman et al.
    <br><b>AIES 2025</b>
  </li>
  
  <li>
    <a href="https://openreview.net/forum?id=nLJZh4M6S5"><b>Reliable and Responsible Foundation Models</b></a>
    <br>Xinyu Yang et al.
    <br><b>TMLR 2025 (Outstanding Survey Paper)</b>
  </li>
  
  <li>
    <a href="https://arxiv.org/abs/2501.17805"><b>International AI Safety Report</b></a>
    <br>Yoshua Bengio et al.
    <br><b>France AI Action Summit</b>
  </li>

  <li><a href="https://arxiv.org/abs/2503.16861"><b>In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI</b></a>
    <br>Shayne Longpre et al.
    <br><b>ICML 2025 (Spotlight, top 2.5% of papers)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2410.08385"><b>Language model developers should report train-test overlap</b></a>
    <br>Andy Zhang et al.
    <br><b>ICML 2025 (Spotlight, top 2.5% of papers)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2503.05336"><b>Toward an Evaluation Science for Generative AI Systems</b></a>
    <br>Laura Weidinger*, Inioluwa Deborah Raji*, et al.
    <br><b>National Academy of Engineering 2025</b>
  </li>
  <li><a href="https://arxiv.org/abs/2502.16701"><b>Beyond Release: Access Considerations for Generative AI Systems</b></a>
    <br>Irene Solaiman, Rishi Bommasani et al.
  </li>
  <li><a href="https://arxiv.org/abs/2412.01946"><b>The Reality of AI and Biorisk</b></a>
    <br>Aidan Peppin et al.
    <br><b>FAccT 2025</b>
  </li>
  <li><a href="https://www.science.org/doi/10.1126/science.adp1848"><b>Considerations for Governing Open Foundation Models</b></a>
    <br>Rishi Bommasani et al.
    <br><b>Science 2024</b>
  </li>
  <li><a href="https://arxiv.org/abs/2412.02145"><b>Effective Mitigations for Systemic Risks from General-Purpose AI</b></a>
    <br>Risto Uuk et al.
  </li>
  <li><a href="https://arxiv.org/abs/2406.16746"><b>The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources</b></a>
    <br>Shayne Longpre*, Stella Biderman* et al.
    <br><b>TMLR 2024 (Outstanding Survey Paper)</b>
  </li>
  <li><a href="https://assets.publishing.service.gov.uk/media/66474eab4f29e1d07fadca3d/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf"><b>Interim International Scientific Report on the Safety of Advanced AI</b></a>
    <br>Yoshua Bengio et al.
    <br><b>2024 AI Seoul Summit</b>
  </li>
  <li><a href="https://crfm.stanford.edu/fmti/paper.pdf"><b>The 2024 Foundation Model Transparency Index</b></a>
    <br>Rishi Bommasani*, Kevin Klyman* et al.
    <br><b>TMLR 2024</b>
  </li>
  <li><a href="https://arxiv.org/abs/2403.07918"><b>On the Societal Impact of Open Foundation Models</b></a>
    <br>Sayash Kapoor*, Rishi Bommasani* et al.
    <br><b>ICML 2024 (Oral, top 1.5% of papers)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2403.04893"><b>A Safe Harbor for AI Evaluation and Red Teaming</b></a>
    <br>Shayne Longpre et al.
    <br><b>ICML 2024 (Oral, top 1.5% of papers)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2402.16268"><b>Foundation Model Transparency Reports</b></a>
    <br>Rishi Bommasani et al.
    <br><b>AIES 2024 (Oral, top 1.5% of papers)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2303.15772"><b>Ecosystem Graphs: The Social Footprint of Foundation Models</b></a>
    <br>Rishi Bommasani et al.
    <br><b>AIES 2024</b>
  </li>
  <li><a href="https://arxiv.org/abs/2212.11672"><b>Trustworthy Social Bias Measurement</b></a>
    <br>Rishi Bommasani, Percy Liang
    <br><b>AIES 2024</b>
  </li>
  <li><a href="https://hai.stanford.edu/sites/default/files/2023-11/AI-Regulatory-Alignment.pdf"><b>AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing</b></a>
    <br>Neel Guha*, Christie M. Lawrence* et al.
    <br><b>George Washington Law Review 2024</b>
  </li>
<li><a href="https://arxiv.org/abs/2310.12941"><b> The 2023 Foundation Model Transparency Index </b></a>
  <br> Rishi Bommasani*, Kevin Klyman* et al.
  <br> <b> TMLR 2024 (Outstanding Paper)</b>
</li>
  <li><a href="https://arxiv.org/abs/2307.05862"><b>Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes</b></a>
    <br>Connor Toups*, Rishi Bommasani* et al.
    <br><b>NeurIPS 2023</b>
  </li>
  <li><a href="https://arxiv.org/abs/2305.02440"><b>Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs</b></a>
    <br>Deepak Narayanan et al.
    <br><b>NeurIPS 2023</b>
  </li>
  <li><a href="https://arxiv.org/abs/2212.11670"><b>Evaluation for Change</b></a>
    <br>Rishi Bommasani
    <br><b>ACL 2023</b>
  </li>
  <li><a href="https://arxiv.org/abs/2212.09746"><b>Evaluating Human-Language Model Interaction</b></a>
    <br>Mina Lee et al.
    <br><b>TMLR 2023</b>
  </li>
  <li><a href="https://arxiv.org/abs/2211.09110"><b>Holistic Evaluation of Language Models</b></a>
    <br>Percy Liang*, Rishi Bommasani*, Tony Lee* et al.
    <br><b>TMLR 2023 (Best Paper)</b>
  </li>
  <li><a href="https://arxiv.org/abs/2211.13972"><b>Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?</b></a>
    <br>Rishi Bommasani et al.
    <br><b>NeurIPS 2022</b>
  </li>
  <li><a href="https://arxiv.org/abs/2206.07682"><b>Emergent Abilities of Large Language Models</b></a>
    <br>Jason Wei et al.
    <br><b>TMLR 2022 (Outstanding Survey Paper)</b>
  </li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3531146.3534637"><b>Data Governance in the Age of Large-Scale Data-Driven Language Technology</b></a>
    <br>Yacine Jernite et al.
    <br><b>FAccT 2022</b>
  </li>
  <li><b>The Time Is Now to Develop Community Norms for the Release of Foundation Models</b>
    <br>Percy Liang, Rishi Bommasani, Kathleen A. Creel, Rob Reich
  </li>
  <li><a href="https://arxiv.org/abs/2108.07258"><b>On the Opportunities and Risks of Foundation Models</b></a>
    <br>Rishi Bommasani et al.
  </li>
  <li><a href="https://arxiv.org/abs/2108.10692"><b>Generalized Optimal Linear Orders</b></a>
    <br>Rishi Bommasani
    <br><b>M.S. Thesis, Cornell University</b>
  </li>
  <li><a href="https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf"><b>Intrinsic Evaluation of Summarization Datasets</b></a>
    <br>Rishi Bommasani, Claire Cardie
    <br><b>EMNLP 2020</b>
  </li>
  <li><a href="https://aclanthology.org/2020.acl-main.431/"><b>Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings</b></a>
    <br>Rishi Bommasani, Kelly Davis, Claire Cardie
    <br><b>ACL 2020</b>
    <br>
  </li>
</ul>
